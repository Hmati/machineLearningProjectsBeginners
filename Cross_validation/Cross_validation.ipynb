{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation\n",
    "\n",
    "We have a dataset, the first thing we need to decide on is, what model to use. Example if you were to have a classification problem, you may decide to use a SVM, KNN, logistic regression, Naive bayes and so on. But whiich model is the best for that dataset. The only way we can answer that question is by evaluating the performance of each mode. We do this using the cross validation procedure.\n",
    "\n",
    "Traditionally when we want to create a model, we first train the model will data and then evaluete the model by comparing the truth against the predictions made by the model. There are several ways we could do this.\n",
    "\n",
    "### 1. Train the model with all dataset and test it against the same dataset\n",
    "\n",
    "In this technique we train the model with the whole dataset and test it against the whole dataset. The problem with this is that the model has already seen those datasets we'll train against hence there is no benefit in doing that if we want to evaluate the model. That will be like a teacher testing the student with same exercise questions he gave as home work, the students will get 100%, this does not really test the understanding of the students.\n",
    "\n",
    "### 2. Splitting the dataset into training and testing samples\n",
    "\n",
    "This technique involves splitting the original dataset into training and testing datasets, then we train the model with the training dataset and evaluate it with the testing dataset to test its understanding of the dataset when it comes to unseen data points. Usually we use a larger percent for training the model and a smaller percent to evaulate the model. The issue with this method is that there maybe baisness in the besting and training datasets, where the training or testing sets may have some features that does not exist in the other set, hence the model will not be able to make predictions well in case unseen data points need to be predicted. Randomization of the original dataset before splitting is one solution to this problem but, there is a better way around things.\n",
    "\n",
    "### 3. Using K fold cross validation\n",
    "\n",
    "This is the best way to evaluate a model. How this works is that, you split the origianl dataset into some folds or samples, lets say you split the original dataset into 7 folds. After that you need to run some iterations 7 times, in each iteration you change which fold or subset is used as the testing set while all the other 6 sets will be used as the training set. In each iteration you note the score of the model and late you find the average of all the 7 scores from each iteration, the average score is the general score of that specific model.\n",
    "\n",
    "#### Lets see this in action, first we'll use the split technique on several models and evaluate them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "svm.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)\n",
    "nb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the note book a couple of times\n",
    "\n",
    "Something you might have noticed if you ran the note book a couple of times is that the score of each model differs from the last time you ran it, hence running the models once, we can't conclude which is the best model to use. The solution is to run this a couple of times and find the average score of each model. That is still not the best practice and its also time consuming. Lets use the k fold cross validation to see how it can solve this issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the K fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(KFold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set:[ 3  4  5  6  7  8  9 10 11 12 13 14]  test: [0 1 2]\n",
      "train set:[ 0  1  2  6  7  8  9 10 11 12 13 14]  test: [3 4 5]\n",
      "train set:[ 0  1  2  3  4  5  9 10 11 12 13 14]  test: [6 7 8]\n",
      "train set:[ 0  1  2  3  4  5  6  7  8 12 13 14]  test: [ 9 10 11]\n",
      "train set:[ 0  1  2  3  4  5  6  7  8  9 10 11]  test: [12 13 14]\n"
     ]
    }
   ],
   "source": [
    "for train, test in kf.split([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]):\n",
    "    print(f'train set:{train}  test: {test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above output, the kfold split the dataset into 5 main subsets, in each subset it again splits the data into equal portions and uses one part of the portion as testing set. Example in the first row it used 0 1 and 2 as testing sets indices while in the second iteration it used 3, 4 and 5 as testing sets indices.**Note the variables train and test are actually indices** by convention the variable names are train_index and test_index. Check the official documentation for more on KFold.\n",
    "\n",
    "\n",
    "## Other types of K fold cross validation\n",
    "\n",
    "1. **StratifiedKFold**\n",
    "\n",
    "Takes group information into account to avoid building folds with imbalanced class distributions (for binary or multiclass classification tasks).\n",
    "\n",
    "2. **GroupKFold**\n",
    "\n",
    "K-fold iterator variant with non-overlapping groups.\n",
    "\n",
    "3. **RepeatedKFold**\n",
    "\n",
    "Repeats K-Fold n times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets use stratifiedKFolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = StratifiedKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keeping the code try\n",
    "\n",
    "You might have noticed how much we repeat our code when creating each classifier above, lets use an inbuilt class to reduce the number of code we write :) pheww!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(cross_val_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96666667, 0.96666667, 0.96666667, 0.93333333, 1.        ])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(SVC(), data.data, data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96666667, 0.96666667, 0.93333333, 0.9       , 1.        ])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(RandomForestClassifier(), data.data, data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.93333333, 0.96666667, 0.93333333, 0.93333333, 1.        ])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(GaussianNB(), data.data, data.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How the cross_val_score works\n",
    "\n",
    "Its magical that we only provide the data and the model to use then we get make the scores. How does this work under the hood? Lets build our own version of this function so you get an idea of how it works, actions are always better then words.\n",
    "\n",
    "#### Steps\n",
    "\n",
    "1. Create a function that takes in the model, the train and the test datasets and returns the scores of it\n",
    "\n",
    "2. Iterate throught the indices returned from the kfold and call the function we created in each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    return model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "0.8333333333333334\n",
      "0.9333333333333333\n",
      "0.7\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in kf.split(data.data):\n",
    "    X_train, X_test, y_train, y_test = (data.data[train_index], data.data[test_index], \n",
    "                                                                                    data.target[train_index],\n",
    "                                                                                    data.target[test_index])\n",
    "    print(get_score(SVC(), X_train, X_test, y_train, y_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_major(model,x_value, y_value):\n",
    "    outputlist = []\n",
    "    \n",
    "    def get_score(X_train, y_train):\n",
    "        return model.fit(X_train, y_train)\n",
    " \n",
    "    for train_index, test_index in kf.split(x_value):\n",
    "        X_train, X_test, y_train, y_test = (x_value[train_index], x_value[test_index], \n",
    "                                                                                        y_value[train_index],\n",
    "                                                                                        y_value[test_index])\n",
    "\n",
    "        outputlist.append(get_score(X_train, y_train).score(X_test, y_test))\n",
    "    \n",
    "    return np.array(outputlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 1.        , 0.83333333, 0.93333333, 0.7       ])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_score_major(SVC(), data.data, data.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since am returning a numpy array i can use all the aggregate numpy functions such as mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8933333333333333"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_score_major(SVC(), data.data, data.target).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_score_major(RandomForestClassifier(), data.data, data.target).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9466666666666667"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_score_major(GaussianNB(), data.data, data.target).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly we can see how the kfold and be used to evaluate models and select the best one. In this case the Gaussian naive bayes algorith looks best suited for this dataset. We can also use this technique for testing different parameters for different model, a process called **parameter tunning**.\n",
    "\n",
    "Well take a closer look at parameter tunning in another article. If you find this helpful consider subscribing to [my youtube channel](https://www.youtube.com/c/CodeWithPrince)\n",
    "\n",
    "By Prince Krampah"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
